![Markdown Logo](assets/images/algorithms-and-data-structres.jpg)
<small>Visual: Unsplash / Lorenzo Herrera</small>
<br/><br/>

<div align="center">

# **Algorithms and data structres 🧱**
 Learn and master Algorithms, Data structres, Asymptotic analysis, Recursion, Dynamic Programming, Divide and conquer and all kina awesomeness 🧃

<br>


[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity) [![Ask Me Anything !](https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg)](https://github.com/humamaboalraja) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)


</div>

---

<br/>

## - [**Table of content**](#table-of-content)

  - 1 . [Getting started](#getting-started)
  - 2 . [Learning Checklist ✅](#learning-checklist-)
  - ## 3 . [Algorithms Applications & theory](#algorithms-applications-&-theory)
    - 1 . [Characteristics of an Algorithm](#characteristics-of-an-algorithm)
    - 2 . [Asymptotic Analysis](#asymptotic-analysis)
      - 1 . [Time Complexity](#time-complexity)
      - 2 . [Space Complexity](#space-complexity)
      - 3 . [Memory](#memory)
    - 3 . [Asymptotic notations](#asymptotic-notations)
      - 1 . [Big-O notation (O)](#big-O-notation-O)
      - 2 . [Omega notation (Ω)](#omega-notation-ω)
      - 3 . [Theta notation (Θ)](#theta-notation-θ)
      - 4 . [Differences between Big-O, Big-Ω and Big-Θ](#differences-between-big-O,-big-Ω-and-big-Θ)
      - 5 . [Best, Worst, Average Case](#best,-worst,-average-case)
        - [Adding Functions](#adding-functions)
        - [Multiplying Functions](#multiplying-functions)
        - [Properties of Logarithm](#properties-of-logarithm)
        - [Dominance Pecking Order](#dominance-pecking-order)
        - [Same Time Complexity](#same-time-complexity)
    - 4 . [Master Theorem](#algorithm-design-strategies--techniques)
    - 5 . [Algorithm Design Techniques & Strategies](#master-theorem)
    - 6 . [Searching Algorithms](#searching-algorithms)
      - 1 . [Linear Search](#linear-search)
      - 2 . [Binary Search](#binary-search)
      - 3 . [Jump Search](#jump-search)
      - 4 . [A* search algorithm](#a*-search-algorithm)
      - 5 . [Ternary search](#ternary-search)
      - 6 . [Exponential search](#exponential-search)
    - 7 . [Recursion (Recursive Algorithms)](#recursion-recursive-algorithms)
      - 1 . [Differences between Recursion and Iteration](#differences-between-recursion-and-iteration)
      - 2 . [Base case](#base-case) exit condition (when do you stop recursing)
      - 3 . [General (recursive) case](#general-recursive-case) (Recurse)
    - 8 . [Sorting Algorithms](#sorting-algorithms)
      - 1 . [In-place Sorting and Not-in-place Sorting](#in-place-sorting-and-not-in-place-sorting)
      - 2 . [Stable and Not Stable Sorting](#in-place-sorting-and-not-in-place-sorting)
      - 3 . [Adaptive and Non-Adaptive Sorting Algorithm](#adaptive-and-non-adaptive-sorting-algorithm)
      - 3 . [Bubble Sort](#bubble-sort)  
      - 4 . [Selection Sort](#selection-sort)  
      - 5 . [Insertion Sort](#insertion-sort)  
      - 6 . [Merge Sort](#merge-sort)  
      - 7 . [Quick Sort](#quick-sort)  
      - 8 . [Counting Sort](#counting-sort)  
      - 9 . [Radix Sort](#radix-sort)  
      - 10 . [Bucket Sort](#bucket-sort)  
    - 9 . [Graph Algorithms](#graph-algorithms)
      - 1 . [Breadth-first search](#breadth-first-search)
      - 2 . [Depth-first search](#depth-first-search)
      - 3 . [Dijkstra’s shortest path algorithm]()
      - 4 . [Bellman–Ford algorithm](#bellman–ford-algorithm)
    - 10 . [Greedy Algorithms](#greedy-algorithms)
    - 11 . [Divide and Conquer](#divide-and-conquer)
    - 12 . [Dynamic Programming](#dynamic-programming)
    - 13 . [Backtracking](#backtracking)
  - ## 4 . [Data structres](#data-structres)
    - 1  . [What are Data structres?](#what-are-data-structres?)
    - 2  . [Logarithm](#logarithm)
    - 3  . [Difference between Linear and Non-linear Data Structures](#difference-between-linear-and-non-linear-data-structures)
    - 4  . [Arrays](#arrays)
    - 5  . [Linked lists](#linked-lists)
      - 5.1  . [Singly Linked lists](#singly-linked-lists)
      - 5.2  . [Doubly Linked lists](#doubly-linked-lists)
      - 5.2  . [Circular Linked lists](#circular-linked-lists)
    - 6  . [Hash tables](#hash-tables)
    - 7  . [Stacks](#stacks)
    - 8 . [Queues](#queues)
    - 9 . [Trees](#trees)
      - 9.1 . [Binary search trees](#binary-search-trees)
      - 9.2 . [AVL Trees](#avl-trees)
      - 9.3 . [Heaps](#heaps)
    - 10 . [Tries](#tries)
    - 11 . [Graphs](#graphs)
    - 12 . [Strings](#strings)
    - 13 . [Priority Queue](#priority-queue)
    - 14 . [Dictionaries](#dictionaries)
    - 15 . [Matrix](#Matrix)
    
    <br>

    #### **Further Learning Resource**

  - 5 . [Articles 📰](#-articles-)
  - 6 . [Books 📚](#-books-)
  - 7 . [Courses 💻](#-courses-)

---

<br/>

# 1

## **Getting started**

<details open>
  <summary>Let's first get to know why do we need this thing 🥸 | <b>Click to expand</b></summary>
</br>

  <div align="center">

  ![](https://media.giphy.com/media/3o7btPCcdNniyf0ArS/giphy.gif)
  
  <br>

  ## **What are Data structres and Algorithms?**
  
  </div>

  A computer program is a collection of instructions to perform a specific task. For this, a computer program may need to store data, retrieve data, and perform computations on the data.

  A data structure is a named location that can be used to store and organize data. And, an algorithm is a collection of steps to solve a particular problem. Learning data structures and algorithms allow us to write efficient and optimized computer programs. [Source](https://www.programiz.com/dsa)

  and the differences between them are that a Data Structure is about organising and managing data effectively such that we can perform specific operation efficiently, while an Algorithm is a step-by-step procedure to be followed to reach the desired output. ... Steps in an algorithm can use one or many data structure(s) to solve a problem.



  ---

  Algorithms and Data structures go through solutions to standard problems in detail and give you an insight into how efficient it is to use each one of them, and help in understanding the nature of the problem at a deeper level and that's why it's very crucial to master them as a Software Engineer.

  and eventually:
  > it's all about data, and the most efficent way to play with this data 😉

  <div align="center">
  Pumped enough! let's get started!
  </div>



---


</details>
<br/>
<br/>
<br/>







# 2
## **Learning Checklist ✅**

<details>
  <summary>A handy checklist to keep track of your progress, and know when you master your Algorithms and data strucres path 💈. <b>Click to expand</b></summary>
</br>

### **Algorithms**:
Show that you know:

- [ ]  Know how to analyze algorithms
- [ ] A detailed description of the algorithm
    - [ ] Goal of the algorithm
    - [ ] Termination condition
    - [ ] Criteria for determining that the algorithm is correct
    - [ ] Explanation of the steps of the algorithm
- [ ] A specific example illustrating how the algorithm works
- [ ] Detailed calculation of the time complexities, including best, worst and average cases
- [ ] Pseudo-code for complex algorithms (this excludes simple searching and sorting algorithms)
- [ ]  Applied these concepts to searching and sorting algorithms

### **Data structures**:
For each of the data structures listed below, you have to understand their functionality, including common operations and their time complexities; what are their strengths and their limitations and, finally, how they are used in real-world scenarios.

When describing a data structure make sure your analysis includes the following points

- [ ] How does the data structure organize and manipulate data?
- [ ] What operations can you do on it and what are their time complexities and why?
- [ ] What operations can you do on it and what are their time complexities and why?
- [ ] Applications
  
  ---

- [ ]  Arrays
  - [ ] Traversing, Searching, Insertion, Deletion, Size
- [ ]  Stacks
  - [ ]  Push, Pop, is empty, top
- [ ]  Queues
- [ ]  Linked Lists
- [ ]  Hash tables
- [ ]  Graphs
- [ ]  Trees
- [ ]  Tries



</details>

---

<br/>
<br/>





# 3

## **Algorithms Applications & theory**


<br>

<details>

<summary>Algorithms Applications & theory | <b>Click to expand</b></summary>
<br>

<div align="center">

![](https://media.giphy.com/media/6wa5vuYvetU1Jibm13/source.gif)

</div>

---

 <h2>Algorithm</h2> 

noun, UK  /ˈæl.ɡə.rɪ.ðəm/ US  /ˈæl.ɡə.rɪ.ðəm/

>a process, step-by-step procedure or set of rules to be followed in calculations or other problem-solving operations to be executed in a certain order to get the desired output, especially by a computer, and those can be simple processes, such as multiplying two numbers, or a complex operation, such as playing  compressed video file. or a Search engine that uses proprietary algorithms to display the most relevant results from its search index for specific queries. Algorithms are generally created independent of underlying languages, i.e. an algorithm can be implemented in more than one programming language.


**<small>Cambridge dictionary/ Oxford Languages</small>**

<br>
<details>
<summary>Algorithms Applications & theory | <b>Click to expand 🔥</b></summary>

<br>

- **What:** Algorithms are a part of daily life actions, and those daily actions and everything we do is the simplest form to represent what an Algorithm is, e.g Finding you car in a parking lot , cleaning your Apartment, reading a book.
  
  ---

- **How to:** There are no well-defined standards for writing algorithms. Rather, it is problem and resource dependent. Algorithms are never written to support a particular programming code. As we know that all programming languages share basic code constructs like loops (do, for, while), flow-control (if-else), etc. These common constructs can be used to write an algorithm. We write algorithms in a step-by-step manner, but it is not always the case. Algorithm writing is a process and is executed after the problem domain is well-defined. That is, we should know the problem domain, for which we are designing a solution.
  
  ---



- **Applications**: The real power of Algorithms come in form of Digital tools, Softwares or small computer programs like, Compression algorithms in a 3D video game or Searching algorithms in Google Search engine, or sorting algorithms to sort Amazon's products based on their ratings and all the other services and digital tools that you use on daily bases.

___

- **Efficiency**: Not all algorithms are created equal, and the tricky part of an Algorithm is that there are plenty of algorithms that solve the same problem at the end, but one of them is the most efficint one to use in that spesific problem case, so to know which solution to choose and to be able to compare them, these Algorithms most be analyzed, and before before analyuzing there an important thing that you need to know what makes a good algorithm is the two most important criteria which are that it solves a problem and it does so efficintly.

    ---
- **Measuring Efficiency**: so the way we measure the effecincy of an algorithm is through using a scientific mathematical technique called **```Asymptotic analysis```**, which allows algorithms to be compared independently of a particular programming language or hardware, which will next tell us that some algorithms are  more efficient than others.

  ---
  
  <br>

  Some important categories of algorithms, from data structures point of view.

  |# | Details|
  |---|------|
  **Search** | Algorithm to search an item in a data structure.
  **Sort** | Algorithm to sort items in a certain order.
  **Insert** | Algorithm to insert item in a data structure.
  **Update** | Algorithm to update an existing item in a data structure.
  **Delete** | Algorithm to delete an existing item from a data structure.


</details>

</details>

---

<br/>
<br/>







# 3.1
## **Characteristics of an Algorithm**:

</br>

<details>
<summary>Characteristics of an Algorithm | <b>Click to expand</b></summary>

<br>

Not all procedures can be called an algorithm. An algorithm should have the following characteristics.


|No.     |  Details|
|------|-------|
**Unambiguous** | Algorithm should be clear and unambiguous.| Each of its steps (or phases), and their inputs/outputs should be clear and must lead to only one meaning.|
**Input** | An algorithm should have 0 or more well-defined inputs.|
**Output** | An algorithm should have 1 or more well-defined outputs, and should match the desired output.|
**Finiteness** | Algorithms must terminate after a finite number of steps.|
**Feasibility** | Should be feasible with the available resources
**Independent** | An algorithm should have step-by-step directions, which should be independent of any programming code.|
**Correctness** | Every step of the algorithm must generate a correct output.|


<br>



<br>


</details>

---

<br/>
<br/>



# 3.2
## **Asymptotic Analysis**:

</br>


**Asymptotic analysis** of an algorithm refers to defining the mathematical boundation/framing of its run-time performance. 

<details>

<summary>Asymptotic Analysis explanation | <b>Click to expand</b></summary>

<br>

- Using asymptotic analysis, we can very well conclude the best case, average case, and worst case scenario of an algorithm. 


- Complexity analysis is effectively used to determine how "good, efficent, scalable, fits the design case best" an algorithm is and whether it's "better" than another one.

- determining how efficient an algorithm is usually involves finding both the **time** complexity and **space** complexity of an Algorithm.

  ---

  <br>
  <div align="center">

  ![](https://media.giphy.com/media/3o6Yg4GUVgIUg3bf7W/giphy.gif)

  ## **But before that what Asymptotic Analysis is about?**
  
  </div>
  <br>

- First what what asymptotic does really mean?! i'm guessing that you have probably heard of the word "asymptote" if not An asymptote is a "line that continually approaches a given curve but does not meet it at any finite distance, in a simpler way it is line that a curve approaches, as it heads towards infinity like that:

  <div align="center">

  ![](assets/images/asymptotic_notation/asymptotic_analysis/asymptote.png)
  </div>
  <br>
  <br>

- The idea of Asymptotic complexity and looking at an asymptotic behavior is that we want to see how does the graph behave getting into very large inputs (n) values, and this is simply the idea of asymptotic complexity. 
  
- So why don’t we in this case to save some time measure the **elapsed real time**, like for instance measuring how fast your code ran on your machine which can tell you how strong your algorithm is!? Because it's not the effecint approach to approach Complexity analysis.

- in computer science problems are often applied at a grand scale like for instance if we are writing an algorithm to optimize whatever part of **Google’s** search engine it’s going to be used across billions of users, and there will be large inputs to the algorithms. 
  
- Our most important point is to see how does this algorithm behave on the tail end ( as (input) gets very large), cause we can only see the true measure of performance of an algorithm when we have very large data input, and that why asymptotic complexity analysis intrigues us ❤️ 

- So in order to be able to indicate the correct Asymptotic Analysis of an algorithm there are some rules that you need to follow: 

  1. We measure as a function of $n$, and ignore low order terms, and that's why we drop constants, which is the reason too why:

     - 5n<sup>3</sup> + n  − 6 becomes n<sup>3</sup>

     - 8n log n  − <math>60n</math> becomes n log n
     - 2<sup>n</sup> + 3n<sup>4</sup>  − becomes 2<sup>n</sup>
      - because when we say (O(log n), o(n), O(n2),  O(n3), O(2n), o(n!)) we are not actually describing an individual graph, or a case that is based on a constant value, what we are really describing is a class of functions and behaviors, and that’s why these functions will have the same behaviour when we get a very large input 😉
  2. If the function is a product of several factors, any constants can be omitted.




  <br>

## **An example of Asymptotic Behaviour**
  
---

  ```Insertion Sort:``` 2 * n<sup>2</sup>  | ```Merge Sort:``` 50 * n * log(n)
    
  ---

  We have 2 computers: 🖥

  - **Computer A**: runs 10 Billion instructions / second

  - **Computer B**:  runs 10 Million instructions / second

  - **Computer A** is 1000x faster than **Computer B**

  - **Computer A** runs insertion sort, **Computer B** runs merge sort

  - How long will each computer take to sort 10 million numbers?

  - **Computer A**: 5.5 hours
  - **Computer B**: 20 minutes

  A computer that runs **1000x** faster lost horrendously to a computer that runs **1000x** slower than it.

  But the thing is that insertion sort will be faster for an initial amount, but it will lose as the input gets larger (and that's what we care about and what is a true expression of its efficiency).

  ---


</details>
<br/>

</details>

### **Time complexity**:
<details>
  <summary>What time complexity is? | <b>Click to expand</b></summary>
</br>

Time complexity of an algorithm represents the amount of time required by the algorithm to run to completion. Time requirements can be defined as a numerical function T(n), where T(n) can be measured as the number of steps, provided each step consumes constant time.

  - For example, addition of two n-bit integers takes n steps. Consequently, the total computational time is T(n) = c ∗ n, where c is the time taken for the addition of two bits. Here, we observe that T(n) grows linearly as the input size increases.

- The time complexity of an algorithm relates the length of an algorithm's input to the number of steps it takes.

<br>

---

We do not want to give the running time of an algorithm in a time unit because this would mean that it is only comparable
with the same implementations (programming language, compiler, hardware, etc.). Thus, it is given as a function:




### **The RAM (random access machine) Model of Computation**:

Is a Machine-indepdendent algorithm design  depends upon a hypothetical computer called (Random Access Machine) and under this model of computation we are confronted with a computer where:

- Simple logical or arithmetic operations (+, *, =, if, call) are considered to be simple operations that that take one time step
-  Loops and subroutines are complex operations composed of multiple time steps based on the number of iterations
- All memory access takes exactly one time step.

> This model encapsulates the core functionality of computers but does not mimic them completely. For example, an addition operation and a multiplication operation are both worth a single time step, however, in reality it will take a machine more operations to compute a product versus a sum.

- The reason the RAM model makes these assumptions is because doing so allows a balance between simplicity and completely imitating underlying machine, resulting in a tool that is useful in practice.

- The exact analysis of algorithms is a difficult task. It is the nature of algorithm analysis to be both machine and language independent. For example, if your computer becomes twice as fast after a recent update, the complexity of your algorithm still remains the same.



</details>

</br></br>


### **Space complexity and Auxiliary Space**:
<details >
  <summary>What time complexity is? | <b>Click to expand</b></summary>
</br>

**Space complexity**: of an algorithm represents the total amount of memory space required by the algorithm in its life cycle (including the space of input values) and relates to the length of an algorithm's input to the number of storage locations it uses. The space required by an algorithm is equal to the sum of the following two components:

- A fixed part that is a space required to store certain data and variables, that are independent of the size of the problem. For example, simple variables and constants used, program size, etc.

- A variable part is a space required by variables, whose size depends on the size of the problem. For example, dynamic memory allocation, recursion stack space, etc.



It is also expressed asymptotically. Space complexity is a measure of the amount of working storage an algorithm needs.
This amount is, again, dependent on the input size. 

to find space-complexity, it is enough to calculate the space occupied by the variables used in an algorithm/program.



---

**Auxiliary space**: is not the same as space complexity even though they are sometimes (wrongly) used for the same thing. Space complexity is the sum of the auxiliary space and the input
space, **Auxiliary space** refers to the temporary space required by an algorithm to be used allocated by the algorithm during it's execution to solve the problem, with respect to input size. Think of temporary arrays, pointers etc.

```
Space Complexity = Auxiliary Space + Space use by input values
```

Whenever a solution to a problem is written some memory is required to complete. For any algorithm memory may be used for the following:

  1. Variables (This include the constant values, temporary values)
   1. Program Instruction
   2. Execution

  ---

  Example #1:

  We can use the RAM model to determine the number of steps it will take an algorithm to end with an input we choose, but estimating the worst, average, and best case runtime scenerio with the RAM model can be unconvenint.



  ```python
  def even_numbers_avg(array):
    '''
    This function returns either the average of the 
    sum of even numbers, or None.
    '''
    even_sum = 0           #1 time step
    even_count = 0         #1 time step
                  #n times:
    for n in array:           #1 time step
        if n % 2 == 0:        #1 time step
            even_sum += n         #1 time step
            even_count +=1        #1 time step
            
    if even_count > 0:              #1 time step
        return even_sum/even_count     #1 time step
    else:                           #1 time step
        return None                    #1 time step
  ```
  - In the example above, we can try to generalize the time complexity of this algorithm by counting every step, and we will find that in the worst case its time complexity will be:  
  T(n) = 5n + 6
  - Its space complexity will be the size of the array n, plus the two variables we initialize.
  - The problem with the notation we used above is that is difficult to work precisely with it. In the example above we can see that both return statements have been counted as well as every step in the for loop, which is correct for the worst but not for the average and best case scenarios.
  - Since we can approximate an algorithm to a mathematical function, we can also determine its growth as a function of the input and define an upper bound function (Big O), a lower bound function (Big Ω), or both (Big Θ) in order to understand how it grows.
  - In Asymptotic Notation, we only consider the fastest growing term without any multiplicative constant. E.g.: in the example above 
   
    **T(n) = 5n + 6 is O(n)** and not **O(5n)**
  - Constant terms (e.g. returning or printing some value) are ignored
    - (constant terms can make a difference if the compared algorithms have the same running time) an that's why In Asymptotic Notation, we only consider the fastest growing term without any multiplicative constant. E.g.: in the example above **T(n) = 5n + 6** is **O(n)** and not **O(5n)**

  - Space complexity
    - the array takes n units of space, as the length can vary
    - the item variable is constant as the new item will be saved and the old item will be discarded
    - thus, we have a space complexity of Θ(𝑛)
    - if we talk about auxiliary space, the input array would not be counted
      - thus, we have a auxiliary space complexity of Θ(1)

> And this is why the term space complexity cannot be used for auxiliary space complexity.

  ---
<br>

  Example #2: In recursive calls stack space also counts. 
  <br>

   ```python
      def add(number):
          if number <= 0:
              return 0
          return number + add(number-1)
   ```

  Here each call add a level to the stack :
```
  1.  add(4)
  2.    -> add(3)
  3.      -> add(2)
  4.        -> add(1)
  5.          -> add(0)
```

Each of these calls is added to call stack and takes up actual memory.
      
So it takes ```O(n) space```.

However just because you have n calls total doesn’t mean it takes O(n) space.



---

</details>


</br></br>


### **Memory**:
<details>
  <summary>What you need to know about memory? | <b>Click to expand</b></summary>
</br>
</details>

</br>

---

</details>



<br/>
<br/>

</details>


# 3.2

## **Asymptotic Notations**:

**Recap:** as we've mentioned in ther Asymptotic analysis section The **efficiency** of an algorithm depends on the amount of time, storage and other resources required to execute the algorithm, and an algorithm may not have the same performance for different types of inputs. With the increase in the input size, the performance will change, now this efficiency is measured with the help of Asymptotic Notations 🔥

---

<br>

<details>
<summary>Asymptotic mathematical Notations <b> | Click to expand</b></summary>

<br>

**Asymptotic Notations** are Mathematical notations that are used to describe the running time of an algorithm when the input tends towards a particular value or a limiting value.

### for example: 

1. In Bubble sort algorithm which we will get to it later, when the input array is already sorted, the time taken by the algorithm is linear i.e. the best case.

2. But, when the input array is in reverse condition, the algorithm takes the maximum time (**quadratic**) to sort the elements i.e. the worst case.

3. When the input array is neither sorted nor in reverse order, then it takes average time. These durations are denoted using asymptotic notations.

  ---

  ### **Order of Dominance in the Asymptotic Limit**

  some common asymptotic growths, valid for both space and time complexity:

 - Constant: O(1)
 - Logarithmic: O(log(n))
 - Linear: O(n)
 - Quasilinear: O(n⋅log(n))
 - Quadratic: O(n2)
 - Exponential: O(2n)
 - Factorial: O(n!)

To understand the difference between some of the most common time complexities, take a look at thi graph below, were the x-axys represents the size of the input of the functions and the y-axys represents the result of the functions.

![](assets/images/asymptotic_notation/big_o/big-o-notation.png)

<div align="center">
<b>n! >> 2n >> n2 >> n⋅logn >> n >> log(n) >> 1</b>
</div>

It is also clear that an efficient algorithm can really make the difference in terms of time and space efficiency, especially as the input size grows.



  ---
<br>

### There are mainly three asymptotic notations which are 🧃:
<br>

### **Big-O notation (𝑂)**: 🤕 (Asymptotic Upper bound)

<details>
  <summary>Big-O notation (O) with code examples | <b>Click to expand</b></summary>

 <br>


 **Big-O**: notation is the formal way to represent the upper bound of the running time of an algorithm. Thus, It measures the worst case time complexity or the longest amount of time an algorithm can possibly take to complete.

 -  Big O notation is usually understood to describe the **worst-case**, complexity of an algorithm, even though the worst-case complexity might differ from the **average-case** complexity.
 - Variables used in Big O notation denote the sizes of inputs to algorithms. For example, **O(n)**  might be the time complexity of an algorithm that traverses through an array of length **n**; similarly, O(n + m) might be the time complexity of an algorithm that traverses through an array of length **n**  and through a string of length **m**
 
 -  e.g. some sorting algorithms have different time complexities
depending on the layout of elements in their input array. In rare cases, their
time complexity will be much worse than in more common cases. Similarly, an
algorithm that takes in a string and performs special operations on uppercase
characters might have a different time complexity when run on an input string
of only uppercase characters vs. on an input string with just a few uppercase
characters.
- when describing the time complexity of an algorithm, it's helpful somtimers to specify what the time complexity refers to. the average case or to the worst case (e.g., "this algorithm runs in O(nlog(n)) time on average and in $O(n^2)$ time in the worse case").

<br>

  <div align="center">

  ![progra](assets/images/asymptotic_notation/big-o.png)

  <small>Big O.
  | Image source / <a href="https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-o-notation">Khan Academy</a></small>
<br>
<br>

</div>
The Big O notation is useful when we only have upper bound on time complexity of an algorithm. Many times we easily find an upper bound by simply looking at the algorithm.  
For a function g(n), O(g(n)) is given by the relation:

``` math
O(f(n)) = { 
  g(n): 
  there exist positive constants c >  0 and n0
  such that 0 ≤ f(n) ≤ c.g(n) for all n ≥ n0 
}

```

Since Big-o gives the worst-case running time of an algorithm, it is widely used to analyze an algorithm as we are always interested in the worst-case scenario.


Summary:
>The Big O of a function is its asymptotic upper bound. This means that the running time of a function T will be always shorter than that of f . To generalize we can say that a funciton **T(n)** is **O(f(n))** if there is a constant k such that **T(n) < k** ⋅ **f(n)** for large enough **n**.



  
    
  </br>
    

  1. ### **Big O cheatsheet**


      <details>
      <summary>Big-O Complexity table ✨ | <b>Click to expand</b></summary>
      </br>




      The following are examples of common complexities and their Big O notations,ordered from fastest to slowest:

      Big O Notation	| Name | Example(s) | Efficiency | Code example|
      |----------------|------|-----------| -------| ----|
      O(1) | Constant | 	Odd or Even number, <br> Look-up table (on average) | 🟩 | Python, Javascript
      O(log(n)) | Logarithmic | Finding element on sorted array with binary search | 🟩 | Python, Javascript
      O(n) | Linear | Find max element in unsorted array. <br> Duplicate elements in array with Hash Map | 🟩 | Python, Javascript
      O(nlog(n)) | Linearithmic | Python, Javascriptorting elements in array with merge sort | 🟩 | Python, Javascript
      O(n<sup>2</sup>) | Quadratic | # Duplicate elements in array **(naïve)**, <br> Sorting array with bubble sort | 🟨 | Python, Javascript
      O(n<sup>3</sup>) | Cubic | 3 variables equation solver | 🟨 | Python, Javascript
      O(2<sup>n</sup>) | Exponential | Find all subsets | 🟥 | Python, Javascript
      O(n!) | Factorial | Find all permutations of a given set/string | 🟥 | Python, Javascript
      </details>

      ---
  <br>

</details>

  ---
  <br>

  ### **Omega notation (Ω)**: 😌 (Asymptotic Lower bound)

  <details>
    <summary>What is Omega notation (Ω) | <b>Click to expand</b></summary>
    </br>
  </details>

  ---

  <br>

  ### **Theta notation (Θ)**: 💈 (Asymptotic Tight (exact) bound))
  <details>
    <summary>What Asymptotic Analysis is? | <b>Click to expand</b></summary>
    </br>
  </details>
  
  ---
<br/>



<br/>


  ## **Differences between Big-O, Big-Ω and Big-Θ**

<details>

<summary>a table that explains the diffrerences between most common asymptotic notations</summary>

  <br>

  Big Oh | Big Omega | Big Theta 
  --------|-------------|--------|
  (It is like <=)  rate of growth of an algorithm is less than or equal to a specific value. | It is (like >=) rate of growth is greater than or equal to a specified value | (It is like ==) meaning the rate of growth is equal to a specified value.
  The upper bound of algorithm is represented by Big O notation. Only the above function is bounded by Big O. asymptotic upper bond is it given by Big O notation.| The algorithm’s lower bound is represented by Omega notation. The asymptotic lower bond is given by Omega notation. | The bounding of function from above and below is represented by theta notation. The exact asymptotic behavior is done by this theta notation.
  Big oh (O) – Worst case	| Big Omega (Ω) – Best case | Big Theta (Θ) – Average case
  Big-O is a measure of the longest amount of time it could possibly take for the algorithm to complete. | Big- Ω is take a small amount of time as compare to Big-O it could possibly take for the algorithm to complete. | Big- Θ is take very short amount of time as compare to Big-O and Big-? it could possibly take for the algorithm to complete.
  Mathematically – Big Oh is ```0 <=f(n) <= c g(n) for all n>=n0```	| Mathematically – Big Omega is ```O<= C g(n) <= f(n) for all n>=n 0```	| Mathematically – Big Theta is ```O<=C 2 g(n)<=f(n)<=C 1 g(n) for n>=n 0```

</details>


</details>

<br/>
<br/>



<br/>





# 3.3

## **Algorithm design strategies & Techniques**:

<details>
  <summary>What Asymptotic Analysis is? | <b>Click to expand</b></summary>
</br>

</details>
<br/>

---

<br/>
<br/>
<br/>
<br/>




# 3.4

## **Algorithm design strategies & Techniques**:

<details>
  <summary>What Asymptotic Analysis is? | <b>Click to expand</b></summary>
</br>

</details>
<br/>

---

<br/>
<br/>
<br/>
<br/>



# 3.5

## **Searching Algorithms**:

<details >
<summary>Searching Algorithms explanation & examples</summary>

<div align="center">



</div>

<br>

  Not even a single day pass, when we do not have to search for something in our day to day life, car keys, books, pen, mobile charger and what not. Same is the life of a computer, there is so much data stored in it, that whenever a user asks for some data, computer has to search it's memory to look for the data and make it available to the user and that's where come:

  **Searching Algorithms** are designed to check for an element or retrieve an element from any data structure where it is stored. Based on the type of search operation, these algorithms are generally classified into two categories:

  - **Sequential Search**: Lists or arrays are traversed sequentially and every element is checked. For example: Linear Search. (designed to search for an item in unsorted data structures, they check every item in the data structure sequentially.) For example: **Linear Search**.

  - **Interval Search**: These algorithms are specifically designed for searching in sorted data-structures. These type of searching algorithms are much more efficient than Linear Search as they repeatedly target the center of the search structure and divide the search space in half. For Example: Binary Search.


  <br>
  
  ### **Linear Search**: 

  <details >
    <summary>What Linear Search with examples | <b>Click to expand</b></summary>
    </br>

  <div align="center">

  ![](assets/images/algorithms/linear-search.gif)

  <small>Linear Search/ [Programming Simplified](programmingsimplified.com)</small>
  
  </div>

  **Linear search** is a very simple sequential search algorithm search that takes in input an array and a value to search and iterates through the array to search for the value. It usually returns the index of the element (if it is in the array) or −1 (if the element is not in the array).
  


``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>

  1.  Start from the leftmost element of array[ ], and one by one compare target with each element of array[ ] 

  2. If target matches with an element, return the index.

  3. If target doesn’t match with any of elements,return -1.




</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>

  Linear Search ( Array A, Value x)


   - 1: Set i to 1
   - 2: if i > n then go to step 7
   - 3: if A[i] = x then go to step 6
   - 4: Set i to i + 1
   - 5: Go to Step 2 |The linear fashion|
   - 6: Print Element x Found at index i and go to step 8
   - 7: Print element not found
   - 8: Exit




</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>

  Sample input/ Output:
    
``` python
array[] = {10, 20, 80, 30, 60, 50, 110, 100, 130, 170}
target = 110;
```

  Output : 6 | Element target is present at index 6

  ---

``` python
array[] = {10, 20, 80, 30, 60, 50, 110, 100, 130, 170}
target = 175;    
```
  Output : -1 | Element target is not present in array[]


</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>

     

  ```python
    def linearSearch(array, target):
        # Linearly search target in array[] 
        for index in range(len(array)): # o(n) linear time
            # If target is present, which is our input for the function
            if array[index] == target:
                # then return its location 
                return index

        return -1

    listOfItemsToSearchIn = [2,9,35,16,2,7,8,22,35,46,57,68,34,213,4,13] # Size N = 15

    matchedIndex = linearSearch(listOfItemsToSearchIn, 13)

    print(matchedIndex)

    # Result: 15

  ```




</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>


<details>
<summary>Time Complexity Analysis</summary>
<br>


- Looking at the linear search algorithm, we assume that this code would need the sum of one iteration times the length of the array plus the time it takes to set up the for loop and returning a value. 
  
  - (**c1 * n + c2**) Where **c1** is the time for one loop iteration, **n** the array length and **c2** the time of the overhead.  We now can say that the running time grows with **n** 
  
  - which means that this algorithm will search for an element by iterating through the whole array of **n**, so in case if the element is not present or positioned as the last element in the array, then the algorithm will go through the whole array of n elements.

  ---

  ### **<div align="center">Complexity</div>**

- **Average | Worst**: We have seen that In the Average | Worst case, the number of elements through which it needs to iterate also depend on **<i>n</i>** which means that Its time complexity in the average and worst case will therefore be **Θ = O(n)**/ **Θ(𝑛)**:
  - linear complexity =  (will keep on looking until it matches with the given input)


- **Best**: case will be in case the element was found at the first iteration (Was positioned as the first element in the list) thus the time complexity of this algorithm in the best case will be **Ω(1)** / **Θ(1)**. 

  ---

- **Termination**: The algorithm terminates for every input, either when it finds the element it is looking for, ot when it iterates through the whole array.



<br>
</details>

<details>
<summary>Space Complexity Analysis</summary>
<br>
No auxiliary data structures are required by this algorithm


</br>
</details>


</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>

   **c1 * n + c2** |  Where **c1** is the time for one loop iteration, **n** the array length and **c2** the time of the overhead. We now can say that the running time grows with **n**



</details>
<br>
<br>


    
  </details>

  <br>
  
  ---

  

  ### **Binary search**:

 <details >
   <summary>What is Binary search with examples | <b>Click to expand</b></summary>
   </br>

<div align="center">

![](https://d18l82el6cdm1i.cloudfront.net/uploads/bePceUMnSG-binary_search_gif.gif)

<small>Image source/ <a href="https://brilliant.org/wiki/binary-search/">Brilliant</a></small>
</div>


<br>

**Binary search** is an efficient Interval Searching algorithm that searches a sorted list for a desired, or target, element. 

It does this efficiently by halving the search space during each iteration of the program. it finds the middle of the list, asks “is the element I’m looking for larger or smaller than this?” Then it cuts the search space in the list in half and searches only in the left list if the element is smaller, and searches only in the right list if the element is bigger. It repeats this process iteratively or recursively until it finds the element it is looking for (or reports back that the element isn’t in the list at all). The algorithm uses a divide and conquer (or divide and reduce) approach to search.



- It takes a sorted array and the value to search for as an input.
- It works by comparing the target value to the middle element of the array.
-  If the target value is greater than the middle element, the left half of the list is eliminated from the search space, and the search continues in the right half.
- If the target value is less than the middle value, the right half is eliminated from the search space, and the search continues in the left half.
- This process is repeated until the middle element is equal to the target value, or if the algorithm returns that the element is not in the list at all.





> The only limitation is that the array or list of elements must be sorted for the binary search algorithm to work on it.

- it only works on sorted lists
- Faster than linear search

<br>

``` 
 🦶🏽 Steps:
```

<details >
<summary>Table of steps</summary>

<br>

1. Determine the middle element of a sorted list by taking the value of the floor of (low + hight) / 2, where low is the lowest index of the list, and high is the highest index in the list. So in the list [1,2,3,4],2 (since 2 occurs at index 1) would be the middle. In the list [1,2,3,4,5],3 (since 3 occurs at index 2) is the middle.

2. Compare the value of that middle element with the target value.
3. If the target value is equal to the middle element, return that it is true the element is in the list (if the position of the element in the list is desired, return the index as well).
4. If the target value is less than the middle element, eliminate all elements to the right of (and including) the middle element from the search, and return to step one with this smaller search space.
5. If the target value is greater than the middle element, eliminate all the elements to the left of (and including) the middle element from the search, and return to step one with this smaller search space.


</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>

1. Let left = 0 and right = n-1.

2. If left > right, then stop: target is not present in array. Return -1.
3. Compute middle as the average of right and left, rounded down (so that it is an integer).
4. If array[middle] equals target, then stop. You found it! Return middle.
5. If the middle was too low, that is, array[middle] < target, then set left = middle + 1.
6. Otherwise, the middle was too high. Set max = middle - 1.
7. Go back to step 2.

</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>


  Sample input/ Output:
    
``` python
array = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
target = 15

```

  Output : 6 | Element target is present at index 6

  ---

``` python
array[] = {10, 20, 80, 30, 60, 50, 110, 100, 130, 170}
target = 130;    



 # Result: 8

```

</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>


  <details>
  <summary>Recursive implementation</summary>

  ```python
def binarySearchHelper(array, target, left, right):

    # Element is not present in the list
    if left > right:
      return - 1
    

    middle = (left + right) // 2
    potentialMatch = array[middle]

    # If element is present at the middle itself
    if target == potentialMatch:
        return middle
    
    # If element is smaller than middle, then it can only
    # be present in left subarray
    elif target < potentialMatch:
        return binarySearchHelper(array, target, left, middle - 1)
        
    # Else the element can only be present in right subarray
    else:
        return binarySearchHelper(array, target, middle + 1, right)

def binarySearch(array, target):
  return binarySearchHelper(array, target, 0, len(array) - 1)



# Test list
array = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
target = 15

# Function call
result = binarySearch(array, target)

if result != -1:
  print("Element is present at index", str(result))
else:
  print("Element is not present in array")


 # Result: 14

  ```
  </details>

  <details>
  <summary>Iterative implementation</summary>
  
  ```python
def binarySearchHelper(array, target, left, right):
    # check that the subarray is not empty
    while left <= right:
      
      # find the index of the median value
      middle = (left + right) // 2
      potentialMatch = array[middle]

       # return the middle value if it is the one we we're searching for
      if target == potentialMatch:
          return middle
      
      # If element is smaller than middle, then it can only
      # be present in left subarray
      elif target < potentialMatch:
          right = middle - 1
          
      # Else the element can only be present in right subarray
      else:
          left = middle + 1
    
    # if the subarray is empty return -1
    return -1

def binarySearch(array, target):
  return binarySearchHelper(array, target, 0, len(array) - 1)

# Test list
array = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
target = 15

# Function call
result = binarySearch(array, target)

if result != -1:
  print("Element is present at index", str(result))
else:
  print("Element is not present in array")


 # Result: 14

  ```
</details>





</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details >
<summary>Asymptotic analysis</summary>

<br>

<details>
<summary>Time Complexity Analysis</summary>
<br>



<details>
<summary><b>O(log n)</b> explanation</summary>

<br>

When we say the time complexity is log n, we actually mean log2 n, although the base of the log doesn't matter in asymptotic notations, but still to understand this better, we generally consider a base of 2.

Let's first understand what log2(n) means.

```python

Output:
__


Expression: log2(n)
____________

For n = 2:
log2(21) = 1
Output = 1
____________

For n = 4
log2(22) = 2
Output = 2
____________

For n = 8
log2(23) = 3
Output = 3
____________

For n = 256
log2(28) = 8
Output = 8
____________

For n = 2048
log2(211) = 11
Output = 11

```

Now that we know how log2(n) works with different values of n, it will be easier for us to relate it with the time complexity of the binary search algorithm and also to understand how we can find out the number of steps required to search any number using binary search for any value of n.


### **Counting the Number of Steps**


- As we have already seen, that with every incorrect middle, binary search cuts down the list of elements into half. So if we start with 32 elements, after first unsuccessful middle, we will be left with 16 elements.

- So consider an array with 8 elements, after the first unsuccessful, binary search will cut down the list to half, leaving behind 4 elements, then 2 elements after the second unsuccessful guess, and finally only 1 element will be left, which will either be the target or not, checking that will involve one more step. So all in all binary search needed at most 4 guesses to search the target in an array with 8 elements.

- If the size of the list would have been 16, then after the first unsuccessful guess, we would have been left with 8 elements. And after that, as we know, we need atmost 4 guesses, add 1 guess to cut down the list from 16 to 8, that brings us to 5 guesses.

- So we can say, as the number of elements are getting doubled, the number of guesses required to find the target increments by 1.
 
 Seeing the pattern, right? Generalizing this, we can say, for an array with n elements,

- the number of times we can repeatedly halve, starting at n, until we get the value 1, plus one.
And guess what, in mathematics, the function log2 n means exactly same. We have already seen how the log function works above, did you notice something there?

- For n = 8, the output of log2 n comes out to be 3, which means the array can be halved 3 times maximum, hence the number of steps(at most) to find the target value will be (3 + 1) = 4.

</details>

---

</br>

  In each iteration, the search space is getting divided by 2. That means that in the current iteration you have to deal with half of the previous iteration array.
  And the above steps continue till left<right
  Best case could be the case where the first mid-value get matched to the element to be searched.


  ---

  ### **<div align="center">Complexity</div>**

- This shows that the time complexity of binary search is 𝑂(𝑙𝑜𝑔𝑛). This is true for both the worst and average case. Best
case would be 𝑂(1) if the first guess is the searched item.

- **Best case**: in Big-Omega notation would Ω(1) as well. Worst-case would be Ω(𝑙𝑜𝑔𝑛), as it would take at least logn many iterations to say that the item is not present.
- We can see that **Big-O** and **Big-Omega** notation have the same values. This means that we can express the running time in Big-Theta notation. Best case is Θ(1) and worst and average case are Θ(𝑙𝑜𝑔𝑛).

  - Best Time Complexity: O(1)
  - Average Time Complexity: O(logn)
  - Worst Time Complexity: O(logn)

  ---


<br>
</details>

<details>
<summary>Space Complexity Analysis</summary>
<br>

No auxiliary space is required in Binary Search implementation
- Hence space complexity: O(1)


</br>
</details>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details >
<summary>Algorithm's mathematical explanation</summary>

<br>

We know that at each step of the algorithm, our search space reduces to half. That means if initially, our search space contains n elements, then after one iteration it contains n/2, then n/4 and so on:

```n —> n/2 —> n/4 —> … —> 1```

Suppose our search space is exhausted after <i>k</i> steps. Then,

``` 
T(n) = T(n/2) + c 

n/2k = 1
n = 2k
k = log2n

T(n) = log(n) = O(logn)
```

> Each time we double (N) we increese the time complexity by one step, where is in an linear case algorithm like linear search, we increese one step at each step we give

Therefore, the time complexity of the binary search algorithm is O(log<sup>2</sup>n), which is very efficient. The auxiliary space required by the program is O(1) for iterative implementation and O(log<sup>2</sup>n)  for recursive implementation due to call stack.


The time complexity of Binary Search can be written as


The above recurrence can be solved either using Recurrence T ree method or Master method. It falls in case II of Master Method and solution of the recurrence is Theta(Logn).

Auxiliary Space: O(1) in case of iterative implementation. In case of recursive implementation, O(Logn) recursion call stack space.



</details>
<br>
<br>
 </details>

   <br>
  
  ---

  

  ### **Jump Search**: 

 <details>
   <summary>What Jump Search with examples | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br> 
 </details>


   <br>
  
  ---

  

  ### **A<sup>*</sup> Search Algorithm**: 

 <details>
   <summary>What is A* Search Algorithm | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---

  

  ### **Ternary search**: 

 <details>
   <summary>What is Ternary search with examples | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---

  

  ### **Exponential search**: 

 <details>
   <summary>What is Exponential search with examples | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

  <br>
<br/>





</div>
</details>

---

<br/>
<br/>
<br/>
<br/>



# 3.5

## **Recursion (Recursive Algorithms)**:
  

<details>
<summary>Recursion cases, explanation & examples</summary>

  <br>

  ### **Differences between Recursion and Iteration**:
  <details>
    <summary> | <b>Click to expand</b></summary>
    </br>

  Property | Recursion | Iteration
  ---------|-----------|------------
  **Definition**|	Function calls itself.|	A set of instructions repeatedly executed.
  **Application**|	For functions.|	For loops.
  **Termination**|	Through base case, where there will be no function call.|	When the termination condition for the iterator ceases to be satisfied.
  **Usage**|	Used when code size needs to be small, and time complexity is not an issue.|	Used when time complexity needs to be balanced against an expanded code size.
  **Code Size** |	Smaller code size|	Larger Code Size.
  **Time Complexity**|	Very high(generally exponential) time complexity.|	Relatively lower time complexity(generally polynomial-logarithmic).|



  </details>

  ---

  <br>

  ### **Base case**:
  <details>
    <summary>Differences between Recursion and Iteration | <b>Click to expand</b></summary>
    </br>
  </details>

  ---

  <br>

  ### **General (recursive) case**:
  <details>
    <summary>Differences between Recursion and Iteration | <b>Click to expand</b></summary>
    </br>

  ---

  </details>




<br/>


</details>

---

<br/>
<br/>
<br/>



# 3.6


## **Sorting Algorithms**:

<details open>
<summary>Sorting Algorithms explanation and examples</summary>
<br>

## **<div align="center">Sorting</div>**

 **Sorting** refers to arranging data in a particular format. Sorting algorithm specifies the way to arrange data in a particular order. Most common orders are in numerical or lexicographical order.
 
 - The importance of sorting lies in the fact that data searching can be optimized to a very high level, if data is stored in a sorted manner. Sorting is also used to represent data in more readable formats. Following are some of the examples of sorting in real-life scenarios −

  ---
  <br>

### **In-place Sorting and Not-in-place Sorting**:

<br>

Sorting algorithms may require some extra space for comparison and temporary storage of few data elements. These algorithms do not require any extra space and sorting is said to happen in-place, or for example, within the array itself. This is called in-place sorting. Bubble sort is an example of in-place sorting.

However, in some sorting algorithms, the program requires space which is more than or equal to the elements being sorted. Sorting which uses equal or more space is called not-in-place sorting. Merge-sort is an example of not-in-place sorting.



  ---
  <br>

### **Stable and Not Stable Sorting**:

![](assets/algorithms/sorting/stable_vs_unstable.png)

A sorting algorithm is said to be stable if two objects with equal keys appear in the same order in sorted output as they appear in the input array to be sorted. in other words The stability of a sorting algorithm is concerned with how the algorithm treats equal (or repeated) elements. Stable sorting algorithms preserve the relative order of equal elements, while unstable sorting algorithms don’t. In other words, stable sorting maintains the position of two equals elements relative to one another.


- We don’t always need stable sorting. Stability is not a concern if:

  - equal elements are indistinguishable
  - all the elements in the collection are distinct
- When equal elements are distinguishable, stability is imperative.  For instance, if the collection already has some order, then sorting on another key must preserve that order.



- Several common sorting algorithms are stable by nature, such as Merge Sort, Timsort, Counting Sort, Insertion Sort, and Bubble Sort. Others such as Quicksort, Heapsort and Selection Sort are unstable.

- We can modify unstable sorting algorithms to be stable. For instance, we can use extra space to maintain stability in Quicksort.


> Recap: If a sorting algorithm, after sorting the contents, does not change the sequence of similar content in which they appear, it is called stable sorting.
>If a sorting algorithm, after sorting the contents, changes the sequence of similar content in which they appear, it is called unstable sorting.



  ---
  <br>

### **Adaptive and Non-Adaptive Sorting Algorithm**:

A sorting algorithm is said to be adaptive, if it takes advantage of already 'sorted' elements in the list that is to be sorted. That is, while sorting if the source list has some element already sorted, adaptive algorithms will take this into account and will try not to re-order them.

A non-adaptive algorithm is one which does not take into account the elements which are already sorted. They try to force every single element to be re-ordered to confirm their sortedness.




  ---
  <br>

<!-- ### **Important Terms**: -->

### **Sorting Efficiency**:

Since the beginning of the programming age, computer scientists have been working on solving the problem of sorting by coming up with various different algorithms to sort data.

The two main criterias to judge which algorithm is better than the other have been:

Time taken to sort the given data.
Memory Space required to do so.

The following questions help to decide which algorithm should be used:

- How many keys will you be sorting?
- Will there be duplicate keys in the data?
- What do you know about your data?
  -Has the data already been partially sorted?
  - Do you know the distribution of the keys?
  - Are your keys very long or hard to compare?
  - Is the range of possible keys very small?
- Do I have to worry about disk accesses?
- How much time do you have to write and debug your routine?

<small>Chapter 14.1 in The Algorithm Design Manual</small>

<br>
<br>

### Most popular Sorting Algorithms time complexities


Algorithm |	  Best	|Average	|Worst|
------------|----------|---|--|
Selection Sort |	Ω(n^2)	|θ(n^2)	|O(n^2)|
Bubble Sort |	Ω(n)	|θ(n^2)	|O(n^2)	|
Insertion Sort |	Ω(n)	|θ(n^2)	|O(n^2)	|
Heap Sort |	Ω(n log(n))	|θ(n log(n))	|O(n log(n))|
Quick Sort|Ω(n log(n))|θ(n log(n))|O(n^2)	|
Merge Sort|Ω(n log(n))|θ(n log(n))|O(n log(n))|	 
Bucket Sort|Ω(n+k)|θ(n+k)	|O(n^2)|
Radix Sort |Ω(nk)	|θ(nk)|O(nk)|

---
  
<br>

<br>

  ### **Bubble Sort**: 

  <details>
    <summary>What is Bubble Sort with examples | <b>Click to expand</b></summary>
    </br>


![](https://upload.wikimedia.org/wikipedia/commons/d/d3/Bubblesort_Animation.gif)

<small>Source/ <a href="https://de.wikipedia.org/wiki/Datei:Bubblesort_Animation.gif">Wikipedia</a></small>

  The Bubble sort algorithm compares each pair of elements in an array and swaps them if they are out of order until the entire array is sorted. For each element in the list, the algorithm compares every pair of elements. 
  
  > it starts by comparing the first element in the array to the next. If the first element is bigger than the next, it swaps them before moving to the next element and repeats the process, until it gets to the end of the array. It then starts a new iteration with one less item to be compared, until the array is sorted.

- Because at each iteration this algorithm moves the item with the largest value at the end, the relative order of elements with the same value remains the same and thus is algorithm is stable.

- This algorithm terminates for every input, as soon as there is an iteration in which no elements of the array need to be sorted.





<details>

<summary>Example:</summary>

<br>

First Pass:
- [ 5 1 4 2 8 ] –> [ 1 5 4 2 8 ], Here, algorithm compares the first two elements, and swaps since 5 > 1.
- [ 1 5 4 2 8 ] –>  [ 1 4 5 2 8 ], Swap since 5 > 4
- [ 1 4 5 2 8 ] –>  [ 1 4 2 5 8 ], Swap since 5 > 2
- [ 1 4 2 5 8 ] –> [ 1 4 2 5 8 ], Now, since these elements are already in order [8 > 5), alg]rithm does not swap them.

Second Pass:
- [ 1 4 2 5 8 ] –> [ 1 4 2 5 8 ]
- [ 1 4 2 5 8 ] –> [ 1 2 4 5 8 ], Swap since 4 > 2
- [ 1 2 4 5 8 ] –> [ 1 2 4 5 8 ]
- [ 1 2 4 5 8 ] –>  [ 1 2 4 5 8 ]
Now, the array is already sorted, but our algorithm does not know if it is completed. The algorithm needs one whole pass without any swap to know it is sorted.

Third Pass:
- [ 1 2 4 5 8 ] –> [ 1 2 4 5 8 ]
- [ 1 2 4 5 8 ] –> [ 1 2 4 5 8 ]
- [ 1 2 4 5 8 ] –> [ 1 2 4 5 8 ]
- [ 1 2 4 5 8 ] –> [ 1 2 4 5 8 ]

</details>

<br><br>



``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>

- Compare A[0] and A[1]. If A[0] is bigger than A[1], swap the elements.
- Move to the next element, A[1] (which might now contain the result of a swap from the previous step), and compare it with A[2]. If A[1]A[1] is bigger than A[2], swap the elements. Do this for every pair of elements until the end of the list.

- Do steps 1 and 2 <i>n</i> times.

  ---

  - The Bubble sort makes multiple passes through a list
  - It compares adjacent items and exchanges those that are out of order
  - Each pass through the list places the next largest value in its proper place
  - Each item "bubbles" up to the location where it belongs

</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>

Non-optimized

```python
procedure bubbleSort( list : array of items )

   loop = list.count;
   
   for i = 0 to loop-1 do:
      swapped = false
		
      for j = 0 to loop-1 do:
      
         /* compare the adjacent elements */   
         if list[j] > list[j+1] then
            /* swap them */
            swap( list[j], list[j+1] )		 
            swapped = true
         end if
         
      end for
      
      /*if no number was swapped that means 
      array is sorted now, break the loop.*/
      
      if(not swapped) then
         break
      end if
      
   end for
   
end procedure return list

```


Optimized

```python
procedure bubbleSort(A : list of sortable items)
    n := length(A)
    repeat
        swapped := false
        for i := 1 to n - 1 inclusive do
            if A[i - 1] > A[i] then
                swap(A[i - 1], A[i])
                swapped = true
            end if
        end for
        n := n - 1
    until not swapped
end procedure

```

</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>


  Sample input/ Output:
    
``` python
array = [8, 5, 2, 9, 5, 6, 3]
```

  Output : [2, 3, 5, 5, 6, 8, 9] array is sorted;




</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>


<details>
<summary>Non-optimized</summary>

<br>


```Python

def bubbleSort(array):
  isSorted = False

  while not isSorted:
    isSorted = True
    for i in range(len(array) - 1):
      if array[i] > array[i + 1]:
        swap(i, i + 1, array)
        isSorted = False
  return array

def swap(i, j, array):
  array[i], array[j] = array[j], array[i]

array = [8, 5, 2, 9, 5, 6, 3]
print(bubbleSort(array))


```

</details>

<details>
<summary>Optimized</summary>

<br>
The above function always runs O(n^2) time even if the array is sorted. It can be optimized by stopping the algorithm if inner loop didn’t cause any swap.



```Python

def bubbleSort(array):

  # check if any items have been swapped
  isSorted = False
  
  # will iterate until the end of the n-1
  counter = 0

  # Run loops two times: one for walking throught the array
  # and the other for comparison
  
  while not isSorted:
    # isSorted keeps track of swapping
    isSorted = True

    # Traverse through all array elements
    #traverse the array from 0 to n-1-counter. Swap if the element found is greater than the next element
    for i in range(len(array) - 1 - counter):
      # To sort in descending order, change > to < in this line.
      if array[i] > array[i + 1]:
        # Swap if greater is at the rear position
        swap(i, i + 1, array)
        isSorted = False
    # will disregard the last valuesorted key in the list cause the counter is 1 now
    counter += 1
  return array

def swap(i, j, array):
  array[i], array[j] = array[j], array[i]

array = [8, 5, 2, 9, 5, 6, 3]
print(bubbleSort(array))


```

</details>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details open>
<summary>Asymptotic analysis</summary>

<br>

<details open>
<summary>Time Complexity Analysis</summary>
<br>


- **Worst Case Complexity**: O(n2)
If we want to sort in ascending order and the array is in descending order or the array is completly unsorted then, the worst case occurs.
- **Best Case Complexity**: O(n)
If the array is already sorted, then there is no need for sorting.
- **Average Case Complexity**: O(n2)
It occurs when the elements of the array are in jumbled order (neither ascending nor descending) | ```or there is only one value that needs to be sorted```.
- **Sorting In Place**: Yes

- **Stable**: Yes


To calculate the complexity of the bubble sort algorithm, it is useful to determine how many comparisons each loop performs. For each element in the array, bubble sort does n-1 comparisons. In big O notation, bubble sort performs O(n) comparisons. Because the array contains nn elements, it has an O(n) number of elements. In other words, bubble sort performs O(n) operations on an O(n) number of elements, leading to a total running time of O(n^2)


Note: O(n) is the best-case running time for bubble sort. It is possible to modify bubble sort to keep track of the number of swaps it performs. If an array is already in sorted order, and bubble sort makes no swaps, the algorithm can terminate after one pass. With this modification, if bubble sort encounters a list that is already sorted, it will finish in O(n) time.



</details>


<details>
<summary>Space Complexity Analysis</summary>
<br>

Because this is an in-place algorithm, it does not require any auxiliary space.



</details>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>

Cycle	| Comparisons
-----|------
1st|	(n-1)|
2nd|	(n-2)|
3rd|	(n-3)|
.......| 	......
last |	1

Here, number of comparisons

```(n - 1) + (n - 2) + (n - 3) +.....+ 1 = n(n - 1) / 2```

- nearly equals to n2

- Hence, Complexity: O(n2)

- Also, if we simply observe the number of loops. The algorithm implements two loops. Hence, the complexity is ```n*n = n2```




</details>
<br>

  </details>

  <br>
  
  ---

  

  ### **Selection Sort**:

 <details>
   <summary>What is Selection Sort with examples<b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

   <br>
  
  ---

  

  ### **Insertion Sort**: 

 <details>
   <summary>What is Insertion Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---

  

  ### **Merge Sort**: 

 <details>
   <summary>What is Merge Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---




  ### **Quick Sort**: 

 <details>
   <summary>What is Quick Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

   <br>
  
  ---



  ### **Radix Sort**: 

 <details>
   <summary>What is Radix Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---



  ### **Counting Sort**: 

 <details>
   <summary>What is Radix Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

   <br>
  
  ---

  

  ### **Bucket Sort**: 

 <details>
   <summary>What is Bucket Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

   <br>
</details>

  ---
 
 
  <br>
<br/>
<br/>
<br/>


# 3.7

## **Graph Algorithms**:

  <br>

<details>
<summary>Graph Algorithms explanation and examples</summary>
<br>

---
  


  ### **Breadth-first search**:
  <details>
    <summary>what is Breadth-first search with examples | <b>Click to expand</b></summary>
    </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>
  </details>

  ---

  <br>

  ### **Depth-first search**:
  <details>
    <summary>what is Depth-first search with examples | <b>Click to expand</b></summary>
    </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>
  </details>

  ---

  <br>

  ### **Dijkstra’s shortest path algorithm**:
  <details>
    <summary>what is Dijkstra’s shortest path algorithm with examples | <b>Click to expand</b></summary>
    </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>


  </details>


  ---

  <br>

  ### **Bellman–Ford algorithm**:
  <details>
    <summary>what is Bellman–Ford algorithm with examples | <b>Click to expand</b></summary>
    </br>

``` 
 🦶🏽 Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 🐾 Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 📟 input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 💻 Implementation:
```


<details>
<summary>Python 🐍</summary>

<br>



</details>
<br>
<br>


```
 ⏳ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 🧮 Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

  ---

  </details>




<br/>

</details>

---

  <br>
<br/>
<br/>
<br/>



# 3.8

## **Divide and Conquer**:

  <br>




  ### **Breadth-first search**:
  <details>
    <summary> | <b>Click to expand</b></summary>
    </br>

  </details>

  ---

  <br>
<br/>
<br/>
<br/>


# 3.9

## **Dynamic Programming**:

  <br>




  ### **Breadth-first search**:
  <details>
    <summary> | <b>Click to expand</b></summary>
    </br>

  </details>

  ---

  <br>
<br/>
<br/>
<br/>


# 3.10

## **Backtracking**:

  <br>




  ### **Breadth-first search**:
  <details>
    <summary> | <b>Click to expand</b></summary>
    </br>

  </details>

  ---

<br/>
<br/>
<br/>
<br/>






<div align="center">

# 4

</div>

## **Data structures** 

<br>

<details>
<summary>Data structres | <b>Click to expand</b></summary>

<br>

<div align="center">

![](https://media.giphy.com/media/2UqWA20weXLPRQW5xQ/giphy.gif)

</div>

---

 <h2><b>Data structres</b></h2> 

A data structure is a named location that can be used to store and organize data. And, an algorithm is a collection of steps to solve a particular problem. Learning data structures and algorithms allow us to write efficient and optimized computer programs. 


- Anything that can store data can be called as a data structure, hence Integer, Float, Boolean, Char etc, all are data structures. They are known as Primitive Data Structures.

- We also have some complex Data Structures, which are used to store large and connected data. Some example of Abstract Data Structure are :

  - Linked List
  - Tree
  - Graph
  - Stack, Queue etc.


All these data structures allow us to perform different operations on data. We select these data structures based on which type of operation is required. We will look into these data structures in more details in our later lessons.


Data structures can also be classified on the basis of the following characteristics:

|Characterstic |	Description|
---------------|--------------|
|Linear	| In Linear data structures,the data items are arranged in a linear sequence. Example: Array
|Non-Linear	| In Non-Linear data structures,the data items are not in sequence. Example: Tree, Graph
|Homogeneous	| In homogeneous data structures,all the elements are of same type. Example: Array
|Non-Homogeneous |	In Non-Homogeneous data structure, the elements may or may not be of the same type. Example: Structures
|Static	| Static data structures are those whose sizes and structures associated memory locations are fixed, at compile time. Example: Array
|Dynamic |	Dynamic structures are those which expands or shrinks depending upon the program need and its execution. Also, their associated memory locations changes. Example: Linked List created using pointers


**<small>Cambridge dictionary/ Oxford Languages</small>**

<br>
<details>
<summary>Algorithms Applications & theory | <b>Click to expand 🔥</b></summary>

<br>

- **What:** 
  
  ---



- **Applications**: 

___

- **Efficiency**: 

    ---
- **Measuring Efficiency**: 


</details>

</details>

---

<br/>
<br/>
<br/>
<br/>







# 8
## **Articles 📰**

___

Article           | Provider (Platform) | Used as reference|
--------------------- | -------------- | -------|
[What does ‘Space Complexity’ mean?](https://www.geeksforgeeks.org/g-fact-86/) | Geeksforgeeks | Yes
[Data Structures - Algorithms Basics](https://www.tutorialspoint.com/data_structures_algorithms/algorithms_basics.htm) | Tutorialspoint | Yes
[Difference between Big Oh, Big Omega and Big Theta](https://www.tutorialspoint.com/data_structures_algorithms/asymptotic_analysis.htm) | Tutorialspoint | Yes
[Difference between Recursion and Iteration](https://www.geeksforgeeks.org/difference-between-recursion-and-iteration/) | Geeksforgeeks | Yes
[Difference between Big Oh, Big Omega and Big Theta](https://www.geeksforgeeks.org/difference-between-big-oh-big-omega-and-big-theta/) | Geeksforgeeks | Yes

___
<br/><br/><br/>

# 9
## **Books 📚**
One of the most straight to the point Books 🔥🕹📟
___

Book name           | Provider (Platform) | Author| Skill level |  Cost
--------------------- | -------------- | -------- | ---------- | -----
[The Algorithm Design Manual](https://www.amazon.com/-/en/Steven-S-S-Skiena/dp/1849967202) | Amazon | Steven S S. Skiena | Intermediate | $75.98 |
[Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People](https://www.amazon.com/-/en/Aditya-Bhargava/dp/1617292230) | Amazon | Aditya Bhargava | Beginner | $39.86 |
___

<br/> 

# 10

## **Courses 💻**
----
The most popular courses that teach Algorithms and data structres. Yes for real 🔥🕹📟

Course name           | Provider (Platform) | Duration| Skill level | Course Cost
--------------------- | -------------- | -------- | ---------- | -----
[Intro to Data Structures and Algorithms](https://www.udacity.com/course/data-structures-and-algorithms-in-python--ud513) | [Udacity]([educative.io](https://www.udacity.com/)) | 4 | Beginner | Free |
[Ace the Python Coding Interview](https://www.educative.io/path/ace-python-coding-interview) | [educative](educative.io) | 21h | Beginner |  |

---

<br/><br/><br/>

# 11

<!-- Tables -->
## **Resources**
**Resources** to learn Algorithms and data structres from 🐱
<br/>

---
<br/>

> Learn by Watching/ Doing/ Reading
>
Title | Description
------------ | -------------
[Data Structures Crash Course (Algoexpert.io)](https://www.algoexpert.io/data-structures) | The foundational knowledge you need to ace the coding interviews.
[Asymptotic Analysis: Big-O Notation and More](https://www.programiz.com/dsa/asymptotic-notations) | In this tutorial, you will learn what asymptotic notations are. Also, you will learn about Big-O notation, Theta notation and Omega notation.
[What Is Asymptotic Analysis? And Why Does It Matter? A Deeper Understanding of Asymptotic Bounding.](https://www.youtube.com/watch?v=myZKhztFhzE) | What Asymptotic Analysis Is!
[YouTube channel: Back To Back SWE](https://www.youtube.com/channel/UCmJz2DV1a3yfgrR7GqRtUUA) | A very helpful youtube channel to learn and understand Asymptotic Bounding, logratihms, sorting algorithms etc..
[Animated Algorithms and Data Structures by Chris Laux.](https://www.chrislaux.com/) | A interactive website explaining some sorting algorithms and some data structures.
[Recursion in software development](https://livevideo.manning.com/module/31_3_1/algorithms-in-motion/recursion/recursion?) | To understand recursion you must first understand recursion
---
